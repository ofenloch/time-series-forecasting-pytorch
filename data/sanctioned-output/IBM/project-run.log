All libraries loaded
config["alpha_vantage"]["datafile"] is data/alphavantage_TIME_SERIES_DAILY_ADJUSTED__IBM__data.json
*********** TEST MODE ***********
     disabling all randomness
*********** TEST MODE ***********
Number data points 6333 from 1999-11-01 to 2025-01-02
Train data shape (5050, 20, 1) (5050,)
Validation data shape (1263, 20, 1) (1263,)
Epoch[1/100] | loss train:0.040779, test:0.128964 | lr:0.010000
Epoch[2/100] | loss train:0.008496, test:0.110229 | lr:0.010000
Epoch[3/100] | loss train:0.006672, test:0.108999 | lr:0.010000
Epoch[4/100] | loss train:0.006340, test:0.089704 | lr:0.010000
Epoch[5/100] | loss train:0.006046, test:0.075439 | lr:0.010000
Epoch[6/100] | loss train:0.006068, test:0.098693 | lr:0.010000
Epoch[7/100] | loss train:0.005894, test:0.093038 | lr:0.010000
Epoch[8/100] | loss train:0.005330, test:0.114077 | lr:0.010000
Epoch[9/100] | loss train:0.005638, test:0.088692 | lr:0.010000
Epoch[10/100] | loss train:0.005352, test:0.082169 | lr:0.010000
Epoch[11/100] | loss train:0.005830, test:0.107695 | lr:0.010000
Epoch[12/100] | loss train:0.005714, test:0.111138 | lr:0.010000
Epoch[13/100] | loss train:0.005754, test:0.075302 | lr:0.010000
Epoch[14/100] | loss train:0.005430, test:0.112830 | lr:0.010000
Epoch[15/100] | loss train:0.005148, test:0.084756 | lr:0.010000
Epoch[16/100] | loss train:0.005382, test:0.093574 | lr:0.010000
Epoch[17/100] | loss train:0.005240, test:0.077912 | lr:0.010000
Epoch[18/100] | loss train:0.005982, test:0.069451 | lr:0.010000
Epoch[19/100] | loss train:0.005091, test:0.069959 | lr:0.010000
Epoch[20/100] | loss train:0.005606, test:0.076384 | lr:0.010000
Epoch[21/100] | loss train:0.005414, test:0.091495 | lr:0.010000
Epoch[22/100] | loss train:0.005226, test:0.090926 | lr:0.010000
Epoch[23/100] | loss train:0.005303, test:0.081091 | lr:0.010000
Epoch[24/100] | loss train:0.005426, test:0.078740 | lr:0.010000
Epoch[25/100] | loss train:0.005166, test:0.094498 | lr:0.010000
Epoch[26/100] | loss train:0.005192, test:0.064871 | lr:0.010000
Epoch[27/100] | loss train:0.005515, test:0.046376 | lr:0.010000
Epoch[28/100] | loss train:0.005141, test:0.058336 | lr:0.010000
Epoch[29/100] | loss train:0.005008, test:0.059554 | lr:0.010000
Epoch[30/100] | loss train:0.004928, test:0.048667 | lr:0.010000
Epoch[31/100] | loss train:0.004820, test:0.066022 | lr:0.010000
Epoch[32/100] | loss train:0.005088, test:0.052451 | lr:0.010000
Epoch[33/100] | loss train:0.005004, test:0.058656 | lr:0.010000
Epoch[34/100] | loss train:0.004905, test:0.054022 | lr:0.010000
Epoch[35/100] | loss train:0.005124, test:0.042412 | lr:0.010000
Epoch[36/100] | loss train:0.004906, test:0.043577 | lr:0.010000
Epoch[37/100] | loss train:0.005300, test:0.034771 | lr:0.010000
Epoch[38/100] | loss train:0.005133, test:0.059543 | lr:0.010000
Epoch[39/100] | loss train:0.004845, test:0.025406 | lr:0.010000
Epoch[40/100] | loss train:0.005394, test:0.030040 | lr:0.010000
Epoch[41/100] | loss train:0.004539, test:0.034406 | lr:0.001000
Epoch[42/100] | loss train:0.004388, test:0.031679 | lr:0.001000
Epoch[43/100] | loss train:0.004273, test:0.033160 | lr:0.001000
Epoch[44/100] | loss train:0.004401, test:0.031676 | lr:0.001000
Epoch[45/100] | loss train:0.004398, test:0.033725 | lr:0.001000
Epoch[46/100] | loss train:0.004169, test:0.032245 | lr:0.001000
Epoch[47/100] | loss train:0.004305, test:0.030690 | lr:0.001000
Epoch[48/100] | loss train:0.004169, test:0.031967 | lr:0.001000
Epoch[49/100] | loss train:0.004219, test:0.036866 | lr:0.001000
Epoch[50/100] | loss train:0.004153, test:0.033455 | lr:0.001000
Epoch[51/100] | loss train:0.004315, test:0.038191 | lr:0.001000
Epoch[52/100] | loss train:0.004233, test:0.034981 | lr:0.001000
Epoch[53/100] | loss train:0.004068, test:0.033956 | lr:0.001000
Epoch[54/100] | loss train:0.004375, test:0.029720 | lr:0.001000
Epoch[55/100] | loss train:0.004288, test:0.028373 | lr:0.001000
Epoch[56/100] | loss train:0.004166, test:0.032082 | lr:0.001000
Epoch[57/100] | loss train:0.004324, test:0.034493 | lr:0.001000
Epoch[58/100] | loss train:0.004324, test:0.032756 | lr:0.001000
Epoch[59/100] | loss train:0.004291, test:0.033044 | lr:0.001000
Epoch[60/100] | loss train:0.004329, test:0.036312 | lr:0.001000
Epoch[61/100] | loss train:0.004355, test:0.034460 | lr:0.001000
Epoch[62/100] | loss train:0.004299, test:0.030240 | lr:0.001000
Epoch[63/100] | loss train:0.004177, test:0.033483 | lr:0.001000
Epoch[64/100] | loss train:0.004199, test:0.034089 | lr:0.001000
Epoch[65/100] | loss train:0.004209, test:0.033441 | lr:0.001000
Epoch[66/100] | loss train:0.004141, test:0.029483 | lr:0.001000
Epoch[67/100] | loss train:0.004256, test:0.031810 | lr:0.001000
Epoch[68/100] | loss train:0.004221, test:0.030213 | lr:0.001000
Epoch[69/100] | loss train:0.004274, test:0.031384 | lr:0.001000
Epoch[70/100] | loss train:0.004116, test:0.035476 | lr:0.001000
Epoch[71/100] | loss train:0.004119, test:0.036838 | lr:0.001000
Epoch[72/100] | loss train:0.004347, test:0.032462 | lr:0.001000
Epoch[73/100] | loss train:0.004261, test:0.033915 | lr:0.001000
Epoch[74/100] | loss train:0.004246, test:0.034651 | lr:0.001000
Epoch[75/100] | loss train:0.004253, test:0.038106 | lr:0.001000
Epoch[76/100] | loss train:0.004226, test:0.029940 | lr:0.001000
Epoch[77/100] | loss train:0.004329, test:0.033000 | lr:0.001000
Epoch[78/100] | loss train:0.004292, test:0.037339 | lr:0.001000
Epoch[79/100] | loss train:0.004156, test:0.031375 | lr:0.001000
Epoch[80/100] | loss train:0.004256, test:0.032694 | lr:0.001000
Epoch[81/100] | loss train:0.004146, test:0.032309 | lr:0.000100
Epoch[82/100] | loss train:0.004201, test:0.033019 | lr:0.000100
Epoch[83/100] | loss train:0.004171, test:0.031991 | lr:0.000100
Epoch[84/100] | loss train:0.004012, test:0.031828 | lr:0.000100
Epoch[85/100] | loss train:0.004230, test:0.033606 | lr:0.000100
Epoch[86/100] | loss train:0.004138, test:0.031762 | lr:0.000100
Epoch[87/100] | loss train:0.004097, test:0.031927 | lr:0.000100
Epoch[88/100] | loss train:0.004090, test:0.034172 | lr:0.000100
Epoch[89/100] | loss train:0.004270, test:0.033628 | lr:0.000100
Epoch[90/100] | loss train:0.004112, test:0.033222 | lr:0.000100
Epoch[91/100] | loss train:0.004070, test:0.033365 | lr:0.000100
Epoch[92/100] | loss train:0.004049, test:0.035198 | lr:0.000100
Epoch[93/100] | loss train:0.004070, test:0.034445 | lr:0.000100
Epoch[94/100] | loss train:0.004205, test:0.034637 | lr:0.000100
Epoch[95/100] | loss train:0.003998, test:0.034657 | lr:0.000100
Epoch[96/100] | loss train:0.004026, test:0.034247 | lr:0.000100
Epoch[97/100] | loss train:0.004037, test:0.034175 | lr:0.000100
Epoch[98/100] | loss train:0.004092, test:0.034275 | lr:0.000100
Epoch[99/100] | loss train:0.003935, test:0.032058 | lr:0.000100
Epoch[100/100] | loss train:0.004038, test:0.034923 | lr:0.000100
Predicted close price of the next trading day: 182.1
