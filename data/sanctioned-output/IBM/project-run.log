All libraries loaded
config["alpha_vantage"]["datafile"] is data/alphavantage_TIME_SERIES_DAILY_ADJUSTED__IBM__data.json
*********** TEST MODE ***********
     disabling all randomness
*********** TEST MODE ***********
Number data points 6333 from 1999-11-01 to 2025-01-02
Train data shape (5050, 20, 1) (5050,)
Validation data shape (1263, 20, 1) (1263,)
Epoch[1/100] | loss train:0.033287, test:0.165138 | lr:0.010000
Epoch[2/100] | loss train:0.007931, test:0.140513 | lr:0.010000
Epoch[3/100] | loss train:0.006487, test:0.130598 | lr:0.010000
Epoch[4/100] | loss train:0.006132, test:0.152180 | lr:0.010000
Epoch[5/100] | loss train:0.005784, test:0.113546 | lr:0.010000
Epoch[6/100] | loss train:0.005339, test:0.118520 | lr:0.010000
Epoch[7/100] | loss train:0.006103, test:0.107826 | lr:0.010000
Epoch[8/100] | loss train:0.006155, test:0.108645 | lr:0.010000
Epoch[9/100] | loss train:0.005255, test:0.095231 | lr:0.010000
Epoch[10/100] | loss train:0.005146, test:0.095398 | lr:0.010000
Epoch[11/100] | loss train:0.005425, test:0.101132 | lr:0.010000
Epoch[12/100] | loss train:0.005416, test:0.132137 | lr:0.010000
Epoch[13/100] | loss train:0.005686, test:0.081209 | lr:0.010000
Epoch[14/100] | loss train:0.004812, test:0.085045 | lr:0.010000
Epoch[15/100] | loss train:0.005061, test:0.083729 | lr:0.010000
Epoch[16/100] | loss train:0.005289, test:0.069668 | lr:0.010000
Epoch[17/100] | loss train:0.005256, test:0.077565 | lr:0.010000
Epoch[18/100] | loss train:0.005601, test:0.074204 | lr:0.010000
Epoch[19/100] | loss train:0.004957, test:0.075552 | lr:0.010000
Epoch[20/100] | loss train:0.005096, test:0.066460 | lr:0.010000
Epoch[21/100] | loss train:0.004827, test:0.050749 | lr:0.010000
Epoch[22/100] | loss train:0.004939, test:0.049526 | lr:0.010000
Epoch[23/100] | loss train:0.004980, test:0.075640 | lr:0.010000
Epoch[24/100] | loss train:0.004953, test:0.057189 | lr:0.010000
Epoch[25/100] | loss train:0.005328, test:0.054093 | lr:0.010000
Epoch[26/100] | loss train:0.005038, test:0.079501 | lr:0.010000
Epoch[27/100] | loss train:0.004912, test:0.071344 | lr:0.010000
Epoch[28/100] | loss train:0.005202, test:0.054818 | lr:0.010000
Epoch[29/100] | loss train:0.004908, test:0.056822 | lr:0.010000
Epoch[30/100] | loss train:0.004781, test:0.067072 | lr:0.010000
Epoch[31/100] | loss train:0.004840, test:0.043123 | lr:0.010000
Epoch[32/100] | loss train:0.005219, test:0.059322 | lr:0.010000
Epoch[33/100] | loss train:0.004707, test:0.053228 | lr:0.010000
Epoch[34/100] | loss train:0.004963, test:0.046202 | lr:0.010000
Epoch[35/100] | loss train:0.004821, test:0.060125 | lr:0.010000
Epoch[36/100] | loss train:0.004956, test:0.033867 | lr:0.010000
Epoch[37/100] | loss train:0.005048, test:0.042489 | lr:0.010000
Epoch[38/100] | loss train:0.004860, test:0.040318 | lr:0.010000
Epoch[39/100] | loss train:0.005104, test:0.055128 | lr:0.010000
Epoch[40/100] | loss train:0.004811, test:0.027578 | lr:0.010000
Epoch[41/100] | loss train:0.004346, test:0.035557 | lr:0.001000
Epoch[42/100] | loss train:0.004197, test:0.034465 | lr:0.001000
Epoch[43/100] | loss train:0.004210, test:0.035166 | lr:0.001000
Epoch[44/100] | loss train:0.004276, test:0.037239 | lr:0.001000
Epoch[45/100] | loss train:0.004292, test:0.036715 | lr:0.001000
Epoch[46/100] | loss train:0.004157, test:0.036688 | lr:0.001000
Epoch[47/100] | loss train:0.004283, test:0.036225 | lr:0.001000
Epoch[48/100] | loss train:0.004133, test:0.043505 | lr:0.001000
Epoch[49/100] | loss train:0.004196, test:0.045372 | lr:0.001000
Epoch[50/100] | loss train:0.004102, test:0.042713 | lr:0.001000
Epoch[51/100] | loss train:0.004146, test:0.038256 | lr:0.001000
Epoch[52/100] | loss train:0.004265, test:0.043288 | lr:0.001000
Epoch[53/100] | loss train:0.004198, test:0.039413 | lr:0.001000
Epoch[54/100] | loss train:0.004183, test:0.041508 | lr:0.001000
Epoch[55/100] | loss train:0.004131, test:0.039708 | lr:0.001000
Epoch[56/100] | loss train:0.003966, test:0.046952 | lr:0.001000
Epoch[57/100] | loss train:0.004042, test:0.042473 | lr:0.001000
Epoch[58/100] | loss train:0.004209, test:0.046460 | lr:0.001000
Epoch[59/100] | loss train:0.004171, test:0.046162 | lr:0.001000
Epoch[60/100] | loss train:0.004131, test:0.043995 | lr:0.001000
Epoch[61/100] | loss train:0.004184, test:0.039528 | lr:0.001000
Epoch[62/100] | loss train:0.004089, test:0.046459 | lr:0.001000
Epoch[63/100] | loss train:0.004101, test:0.045048 | lr:0.001000
Epoch[64/100] | loss train:0.004120, test:0.038105 | lr:0.001000
Epoch[65/100] | loss train:0.003957, test:0.043903 | lr:0.001000
Epoch[66/100] | loss train:0.004155, test:0.047850 | lr:0.001000
Epoch[67/100] | loss train:0.004104, test:0.045274 | lr:0.001000
Epoch[68/100] | loss train:0.004059, test:0.046258 | lr:0.001000
Epoch[69/100] | loss train:0.004090, test:0.042994 | lr:0.001000
Epoch[70/100] | loss train:0.004203, test:0.045741 | lr:0.001000
Epoch[71/100] | loss train:0.004292, test:0.055300 | lr:0.001000
Epoch[72/100] | loss train:0.004240, test:0.049655 | lr:0.001000
Epoch[73/100] | loss train:0.003965, test:0.047502 | lr:0.001000
Epoch[74/100] | loss train:0.004146, test:0.046849 | lr:0.001000
Epoch[75/100] | loss train:0.004242, test:0.045035 | lr:0.001000
Epoch[76/100] | loss train:0.004198, test:0.040124 | lr:0.001000
Epoch[77/100] | loss train:0.004182, test:0.041628 | lr:0.001000
Epoch[78/100] | loss train:0.004118, test:0.040618 | lr:0.001000
Epoch[79/100] | loss train:0.004062, test:0.041507 | lr:0.001000
Epoch[80/100] | loss train:0.004062, test:0.047227 | lr:0.001000
Epoch[81/100] | loss train:0.003948, test:0.045696 | lr:0.000100
Epoch[82/100] | loss train:0.004190, test:0.044807 | lr:0.000100
Epoch[83/100] | loss train:0.003989, test:0.044329 | lr:0.000100
Epoch[84/100] | loss train:0.003925, test:0.044981 | lr:0.000100
Epoch[85/100] | loss train:0.003983, test:0.045379 | lr:0.000100
Epoch[86/100] | loss train:0.004010, test:0.046459 | lr:0.000100
Epoch[87/100] | loss train:0.004079, test:0.044914 | lr:0.000100
Epoch[88/100] | loss train:0.004133, test:0.045659 | lr:0.000100
Epoch[89/100] | loss train:0.004115, test:0.045219 | lr:0.000100
Epoch[90/100] | loss train:0.004036, test:0.046570 | lr:0.000100
Epoch[91/100] | loss train:0.004029, test:0.044767 | lr:0.000100
Epoch[92/100] | loss train:0.004042, test:0.044996 | lr:0.000100
Epoch[93/100] | loss train:0.004190, test:0.046233 | lr:0.000100
Epoch[94/100] | loss train:0.004090, test:0.044774 | lr:0.000100
Epoch[95/100] | loss train:0.003995, test:0.043847 | lr:0.000100
Epoch[96/100] | loss train:0.004026, test:0.044828 | lr:0.000100
Epoch[97/100] | loss train:0.004064, test:0.044951 | lr:0.000100
Epoch[98/100] | loss train:0.004080, test:0.046309 | lr:0.000100
Epoch[99/100] | loss train:0.003983, test:0.046684 | lr:0.000100
Epoch[100/100] | loss train:0.003961, test:0.047849 | lr:0.000100
Predicted close price of the next trading day: 175.85
