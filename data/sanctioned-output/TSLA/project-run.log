All libraries loaded
config["alpha_vantage"]["datafile"] is data/alphavantage_TIME_SERIES_DAILY_ADJUSTED__TSLA__data.json
*********** TEST MODE ***********
     disabling all randomness
*********** TEST MODE ***********
Number data points 3653 from 2010-06-29 to 2025-01-02
Train data shape (2906, 20, 1) (2906,)
Validation data shape (727, 20, 1) (727,)
Epoch[1/100] | loss train:0.046920, test:0.010742 | lr:0.010000
Epoch[2/100] | loss train:0.008600, test:0.008280 | lr:0.010000
Epoch[3/100] | loss train:0.006655, test:0.002928 | lr:0.010000
Epoch[4/100] | loss train:0.004619, test:0.004596 | lr:0.010000
Epoch[5/100] | loss train:0.005109, test:0.016770 | lr:0.010000
Epoch[6/100] | loss train:0.006229, test:0.002423 | lr:0.010000
Epoch[7/100] | loss train:0.006267, test:0.006834 | lr:0.010000
Epoch[8/100] | loss train:0.004185, test:0.002023 | lr:0.010000
Epoch[9/100] | loss train:0.003892, test:0.002563 | lr:0.010000
Epoch[10/100] | loss train:0.004806, test:0.003276 | lr:0.010000
Epoch[11/100] | loss train:0.003974, test:0.005324 | lr:0.010000
Epoch[12/100] | loss train:0.006560, test:0.004106 | lr:0.010000
Epoch[13/100] | loss train:0.005787, test:0.008561 | lr:0.010000
Epoch[14/100] | loss train:0.004957, test:0.002196 | lr:0.010000
Epoch[15/100] | loss train:0.004655, test:0.003246 | lr:0.010000
Epoch[16/100] | loss train:0.004377, test:0.006506 | lr:0.010000
Epoch[17/100] | loss train:0.005188, test:0.003941 | lr:0.010000
Epoch[18/100] | loss train:0.004093, test:0.002913 | lr:0.010000
Epoch[19/100] | loss train:0.004097, test:0.002069 | lr:0.010000
Epoch[20/100] | loss train:0.003907, test:0.007291 | lr:0.010000
Epoch[21/100] | loss train:0.003562, test:0.003470 | lr:0.010000
Epoch[22/100] | loss train:0.003921, test:0.002952 | lr:0.010000
Epoch[23/100] | loss train:0.003654, test:0.003979 | lr:0.010000
Epoch[24/100] | loss train:0.004881, test:0.002081 | lr:0.010000
Epoch[25/100] | loss train:0.003495, test:0.002288 | lr:0.010000
Epoch[26/100] | loss train:0.005024, test:0.004304 | lr:0.010000
Epoch[27/100] | loss train:0.003462, test:0.002314 | lr:0.010000
Epoch[28/100] | loss train:0.003920, test:0.003619 | lr:0.010000
Epoch[29/100] | loss train:0.004219, test:0.006405 | lr:0.010000
Epoch[30/100] | loss train:0.003996, test:0.002537 | lr:0.010000
Epoch[31/100] | loss train:0.004093, test:0.002760 | lr:0.010000
Epoch[32/100] | loss train:0.003862, test:0.008568 | lr:0.010000
Epoch[33/100] | loss train:0.004223, test:0.003321 | lr:0.010000
Epoch[34/100] | loss train:0.004312, test:0.005019 | lr:0.010000
Epoch[35/100] | loss train:0.003346, test:0.002853 | lr:0.010000
Epoch[36/100] | loss train:0.003871, test:0.003173 | lr:0.010000
Epoch[37/100] | loss train:0.003405, test:0.002882 | lr:0.010000
Epoch[38/100] | loss train:0.004535, test:0.003480 | lr:0.010000
Epoch[39/100] | loss train:0.003368, test:0.002760 | lr:0.010000
Epoch[40/100] | loss train:0.003598, test:0.007942 | lr:0.010000
Epoch[41/100] | loss train:0.003543, test:0.002181 | lr:0.001000
Epoch[42/100] | loss train:0.003277, test:0.002146 | lr:0.001000
Epoch[43/100] | loss train:0.003158, test:0.002330 | lr:0.001000
Epoch[44/100] | loss train:0.002859, test:0.001758 | lr:0.001000
Epoch[45/100] | loss train:0.002703, test:0.001631 | lr:0.001000
Epoch[46/100] | loss train:0.002968, test:0.001905 | lr:0.001000
Epoch[47/100] | loss train:0.002673, test:0.001840 | lr:0.001000
Epoch[48/100] | loss train:0.002767, test:0.001868 | lr:0.001000
Epoch[49/100] | loss train:0.002917, test:0.002203 | lr:0.001000
Epoch[50/100] | loss train:0.002832, test:0.003157 | lr:0.001000
Epoch[51/100] | loss train:0.003198, test:0.001854 | lr:0.001000
Epoch[52/100] | loss train:0.003114, test:0.001829 | lr:0.001000
Epoch[53/100] | loss train:0.002575, test:0.002665 | lr:0.001000
Epoch[54/100] | loss train:0.003086, test:0.003079 | lr:0.001000
Epoch[55/100] | loss train:0.002579, test:0.001844 | lr:0.001000
Epoch[56/100] | loss train:0.002766, test:0.001644 | lr:0.001000
Epoch[57/100] | loss train:0.002856, test:0.002077 | lr:0.001000
Epoch[58/100] | loss train:0.002594, test:0.002128 | lr:0.001000
Epoch[59/100] | loss train:0.002744, test:0.001839 | lr:0.001000
Epoch[60/100] | loss train:0.002650, test:0.001950 | lr:0.001000
Epoch[61/100] | loss train:0.002621, test:0.002319 | lr:0.001000
Epoch[62/100] | loss train:0.003194, test:0.001990 | lr:0.001000
Epoch[63/100] | loss train:0.002874, test:0.001892 | lr:0.001000
Epoch[64/100] | loss train:0.002778, test:0.002506 | lr:0.001000
Epoch[65/100] | loss train:0.003010, test:0.002015 | lr:0.001000
Epoch[66/100] | loss train:0.002952, test:0.001750 | lr:0.001000
Epoch[67/100] | loss train:0.003369, test:0.002172 | lr:0.001000
Epoch[68/100] | loss train:0.002810, test:0.001780 | lr:0.001000
Epoch[69/100] | loss train:0.002880, test:0.001905 | lr:0.001000
Epoch[70/100] | loss train:0.002875, test:0.001954 | lr:0.001000
Epoch[71/100] | loss train:0.002838, test:0.001935 | lr:0.001000
Epoch[72/100] | loss train:0.002514, test:0.003610 | lr:0.001000
Epoch[73/100] | loss train:0.002801, test:0.001839 | lr:0.001000
Epoch[74/100] | loss train:0.002838, test:0.001942 | lr:0.001000
Epoch[75/100] | loss train:0.002521, test:0.002103 | lr:0.001000
Epoch[76/100] | loss train:0.002838, test:0.002033 | lr:0.001000
Epoch[77/100] | loss train:0.002979, test:0.001841 | lr:0.001000
Epoch[78/100] | loss train:0.003402, test:0.001969 | lr:0.001000
Epoch[79/100] | loss train:0.002754, test:0.002176 | lr:0.001000
Epoch[80/100] | loss train:0.002987, test:0.002007 | lr:0.001000
Epoch[81/100] | loss train:0.003044, test:0.001880 | lr:0.000100
Epoch[82/100] | loss train:0.002830, test:0.001790 | lr:0.000100
Epoch[83/100] | loss train:0.002681, test:0.001948 | lr:0.000100
Epoch[84/100] | loss train:0.002477, test:0.001817 | lr:0.000100
Epoch[85/100] | loss train:0.002532, test:0.001833 | lr:0.000100
Epoch[86/100] | loss train:0.002603, test:0.001697 | lr:0.000100
Epoch[87/100] | loss train:0.002457, test:0.001851 | lr:0.000100
Epoch[88/100] | loss train:0.002706, test:0.001824 | lr:0.000100
Epoch[89/100] | loss train:0.002582, test:0.001991 | lr:0.000100
Epoch[90/100] | loss train:0.002866, test:0.002223 | lr:0.000100
Epoch[91/100] | loss train:0.002788, test:0.001920 | lr:0.000100
Epoch[92/100] | loss train:0.002513, test:0.002190 | lr:0.000100
Epoch[93/100] | loss train:0.002993, test:0.001920 | lr:0.000100
Epoch[94/100] | loss train:0.002853, test:0.001968 | lr:0.000100
Epoch[95/100] | loss train:0.002314, test:0.002405 | lr:0.000100
Epoch[96/100] | loss train:0.002268, test:0.002011 | lr:0.000100
Epoch[97/100] | loss train:0.002756, test:0.002365 | lr:0.000100
Epoch[98/100] | loss train:0.002612, test:0.001818 | lr:0.000100
Epoch[99/100] | loss train:0.002610, test:0.002523 | lr:0.000100
Epoch[100/100] | loss train:0.003126, test:0.001761 | lr:0.000100
Predicted close price of the next trading day: 368.28
