All libraries loaded
config["alpha_vantage"]["datafile"] is data/alphavantage_TIME_SERIES_DAILY_ADJUSTED__GOOGL__data.json
*********** TEST MODE ***********
     disabling all randomness
*********** TEST MODE ***********
Number data points 5128 from 2004-08-19 to 2025-01-02
Train data shape (4086, 20, 1) (4086,)
Validation data shape (1022, 20, 1) (1022,)
Epoch[1/100] | loss train:0.019339, test:0.161930 | lr:0.010000
Epoch[2/100] | loss train:0.002771, test:0.148381 | lr:0.010000
Epoch[3/100] | loss train:0.002262, test:0.138419 | lr:0.010000
Epoch[4/100] | loss train:0.002038, test:0.139417 | lr:0.010000
Epoch[5/100] | loss train:0.002049, test:0.092270 | lr:0.010000
Epoch[6/100] | loss train:0.001597, test:0.133024 | lr:0.010000
Epoch[7/100] | loss train:0.001745, test:0.126690 | lr:0.010000
Epoch[8/100] | loss train:0.001352, test:0.094310 | lr:0.010000
Epoch[9/100] | loss train:0.001626, test:0.087906 | lr:0.010000
Epoch[10/100] | loss train:0.001604, test:0.085822 | lr:0.010000
Epoch[11/100] | loss train:0.001300, test:0.114027 | lr:0.010000
Epoch[12/100] | loss train:0.001454, test:0.078809 | lr:0.010000
Epoch[13/100] | loss train:0.001781, test:0.097856 | lr:0.010000
Epoch[14/100] | loss train:0.001509, test:0.109018 | lr:0.010000
Epoch[15/100] | loss train:0.001450, test:0.075905 | lr:0.010000
Epoch[16/100] | loss train:0.001708, test:0.097922 | lr:0.010000
Epoch[17/100] | loss train:0.001390, test:0.086996 | lr:0.010000
Epoch[18/100] | loss train:0.001407, test:0.076583 | lr:0.010000
Epoch[19/100] | loss train:0.001617, test:0.032392 | lr:0.010000
Epoch[20/100] | loss train:0.001452, test:0.076690 | lr:0.010000
Epoch[21/100] | loss train:0.001420, test:0.097903 | lr:0.010000
Epoch[22/100] | loss train:0.001467, test:0.070989 | lr:0.010000
Epoch[23/100] | loss train:0.001252, test:0.066024 | lr:0.010000
Epoch[24/100] | loss train:0.001405, test:0.054715 | lr:0.010000
Epoch[25/100] | loss train:0.001361, test:0.084252 | lr:0.010000
Epoch[26/100] | loss train:0.001493, test:0.050582 | lr:0.010000
Epoch[27/100] | loss train:0.001379, test:0.058411 | lr:0.010000
Epoch[28/100] | loss train:0.001402, test:0.098035 | lr:0.010000
Epoch[29/100] | loss train:0.001297, test:0.098244 | lr:0.010000
Epoch[30/100] | loss train:0.001299, test:0.070365 | lr:0.010000
Epoch[31/100] | loss train:0.001489, test:0.035731 | lr:0.010000
Epoch[32/100] | loss train:0.001294, test:0.039676 | lr:0.010000
Epoch[33/100] | loss train:0.001365, test:0.052087 | lr:0.010000
Epoch[34/100] | loss train:0.001448, test:0.042421 | lr:0.010000
Epoch[35/100] | loss train:0.001304, test:0.082967 | lr:0.010000
Epoch[36/100] | loss train:0.001412, test:0.020767 | lr:0.010000
Epoch[37/100] | loss train:0.001274, test:0.040080 | lr:0.010000
Epoch[38/100] | loss train:0.001301, test:0.046760 | lr:0.010000
Epoch[39/100] | loss train:0.001332, test:0.031727 | lr:0.010000
Epoch[40/100] | loss train:0.001168, test:0.059574 | lr:0.010000
Epoch[41/100] | loss train:0.001111, test:0.045276 | lr:0.001000
Epoch[42/100] | loss train:0.000999, test:0.052726 | lr:0.001000
Epoch[43/100] | loss train:0.000939, test:0.060826 | lr:0.001000
Epoch[44/100] | loss train:0.001010, test:0.049488 | lr:0.001000
Epoch[45/100] | loss train:0.001015, test:0.049303 | lr:0.001000
Epoch[46/100] | loss train:0.000922, test:0.053149 | lr:0.001000
Epoch[47/100] | loss train:0.000961, test:0.041071 | lr:0.001000
Epoch[48/100] | loss train:0.001030, test:0.043080 | lr:0.001000
Epoch[49/100] | loss train:0.001046, test:0.054552 | lr:0.001000
Epoch[50/100] | loss train:0.001017, test:0.057848 | lr:0.001000
Epoch[51/100] | loss train:0.001029, test:0.057148 | lr:0.001000
Epoch[52/100] | loss train:0.000975, test:0.055490 | lr:0.001000
Epoch[53/100] | loss train:0.000985, test:0.058464 | lr:0.001000
Epoch[54/100] | loss train:0.000998, test:0.061867 | lr:0.001000
Epoch[55/100] | loss train:0.000923, test:0.062831 | lr:0.001000
Epoch[56/100] | loss train:0.000974, test:0.044238 | lr:0.001000
Epoch[57/100] | loss train:0.000986, test:0.061978 | lr:0.001000
Epoch[58/100] | loss train:0.001012, test:0.054538 | lr:0.001000
Epoch[59/100] | loss train:0.000957, test:0.043731 | lr:0.001000
Epoch[60/100] | loss train:0.000988, test:0.045565 | lr:0.001000
Epoch[61/100] | loss train:0.001003, test:0.053134 | lr:0.001000
Epoch[62/100] | loss train:0.000942, test:0.053348 | lr:0.001000
Epoch[63/100] | loss train:0.000968, test:0.061884 | lr:0.001000
Epoch[64/100] | loss train:0.000937, test:0.071577 | lr:0.001000
Epoch[65/100] | loss train:0.001012, test:0.054806 | lr:0.001000
Epoch[66/100] | loss train:0.001000, test:0.046437 | lr:0.001000
Epoch[67/100] | loss train:0.001011, test:0.048495 | lr:0.001000
Epoch[68/100] | loss train:0.000972, test:0.050473 | lr:0.001000
Epoch[69/100] | loss train:0.000985, test:0.033277 | lr:0.001000
Epoch[70/100] | loss train:0.000941, test:0.054708 | lr:0.001000
Epoch[71/100] | loss train:0.000957, test:0.048962 | lr:0.001000
Epoch[72/100] | loss train:0.000990, test:0.045875 | lr:0.001000
Epoch[73/100] | loss train:0.000990, test:0.037865 | lr:0.001000
Epoch[74/100] | loss train:0.000972, test:0.057176 | lr:0.001000
Epoch[75/100] | loss train:0.001028, test:0.033393 | lr:0.001000
Epoch[76/100] | loss train:0.001015, test:0.053189 | lr:0.001000
Epoch[77/100] | loss train:0.000974, test:0.052772 | lr:0.001000
Epoch[78/100] | loss train:0.000933, test:0.039524 | lr:0.001000
Epoch[79/100] | loss train:0.000940, test:0.042832 | lr:0.001000
Epoch[80/100] | loss train:0.000961, test:0.046546 | lr:0.001000
Epoch[81/100] | loss train:0.000944, test:0.048284 | lr:0.000100
Epoch[82/100] | loss train:0.000918, test:0.049595 | lr:0.000100
Epoch[83/100] | loss train:0.000975, test:0.047710 | lr:0.000100
Epoch[84/100] | loss train:0.000905, test:0.048907 | lr:0.000100
Epoch[85/100] | loss train:0.000911, test:0.049829 | lr:0.000100
Epoch[86/100] | loss train:0.000949, test:0.048283 | lr:0.000100
Epoch[87/100] | loss train:0.000992, test:0.045234 | lr:0.000100
Epoch[88/100] | loss train:0.000947, test:0.050425 | lr:0.000100
Epoch[89/100] | loss train:0.000949, test:0.052729 | lr:0.000100
Epoch[90/100] | loss train:0.000946, test:0.055785 | lr:0.000100
Epoch[91/100] | loss train:0.000888, test:0.054363 | lr:0.000100
Epoch[92/100] | loss train:0.000909, test:0.055987 | lr:0.000100
Epoch[93/100] | loss train:0.000929, test:0.055970 | lr:0.000100
Epoch[94/100] | loss train:0.000937, test:0.053268 | lr:0.000100
Epoch[95/100] | loss train:0.000949, test:0.058738 | lr:0.000100
Epoch[96/100] | loss train:0.000938, test:0.053089 | lr:0.000100
Epoch[97/100] | loss train:0.000938, test:0.051173 | lr:0.000100
Epoch[98/100] | loss train:0.000928, test:0.051572 | lr:0.000100
Epoch[99/100] | loss train:0.000913, test:0.053411 | lr:0.000100
Epoch[100/100] | loss train:0.000904, test:0.059034 | lr:0.000100
Predicted close price of the next trading day: 135.91
