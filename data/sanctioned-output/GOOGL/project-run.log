All libraries loaded
config["alpha_vantage"]["datafile"] is data/alphavantage_TIME_SERIES_DAILY_ADJUSTED__GOOGL__data.json
*********** TEST MODE ***********
     disabling all randomness
*********** TEST MODE ***********
Number data points 5128 from 2004-08-19 to 2025-01-02
Train data shape (4086, 20, 1) (4086,)
Validation data shape (1022, 20, 1) (1022,)
Epoch[1/100] | loss train:0.021958, test:0.174295 | lr:0.010000
Epoch[2/100] | loss train:0.003133, test:0.146635 | lr:0.010000
Epoch[3/100] | loss train:0.002457, test:0.120790 | lr:0.010000
Epoch[4/100] | loss train:0.001977, test:0.125962 | lr:0.010000
Epoch[5/100] | loss train:0.001794, test:0.101599 | lr:0.010000
Epoch[6/100] | loss train:0.001568, test:0.079296 | lr:0.010000
Epoch[7/100] | loss train:0.001853, test:0.094951 | lr:0.010000
Epoch[8/100] | loss train:0.001749, test:0.121619 | lr:0.010000
Epoch[9/100] | loss train:0.001882, test:0.082095 | lr:0.010000
Epoch[10/100] | loss train:0.001578, test:0.061606 | lr:0.010000
Epoch[11/100] | loss train:0.001387, test:0.057012 | lr:0.010000
Epoch[12/100] | loss train:0.001375, test:0.060602 | lr:0.010000
Epoch[13/100] | loss train:0.001468, test:0.116876 | lr:0.010000
Epoch[14/100] | loss train:0.001433, test:0.107708 | lr:0.010000
Epoch[15/100] | loss train:0.001569, test:0.053532 | lr:0.010000
Epoch[16/100] | loss train:0.001282, test:0.061326 | lr:0.010000
Epoch[17/100] | loss train:0.001342, test:0.050413 | lr:0.010000
Epoch[18/100] | loss train:0.001386, test:0.046779 | lr:0.010000
Epoch[19/100] | loss train:0.001474, test:0.040898 | lr:0.010000
Epoch[20/100] | loss train:0.001181, test:0.031037 | lr:0.010000
Epoch[21/100] | loss train:0.001529, test:0.037578 | lr:0.010000
Epoch[22/100] | loss train:0.001274, test:0.045549 | lr:0.010000
Epoch[23/100] | loss train:0.001388, test:0.047020 | lr:0.010000
Epoch[24/100] | loss train:0.001333, test:0.032531 | lr:0.010000
Epoch[25/100] | loss train:0.001213, test:0.059062 | lr:0.010000
Epoch[26/100] | loss train:0.001498, test:0.066262 | lr:0.010000
Epoch[27/100] | loss train:0.001399, test:0.050897 | lr:0.010000
Epoch[28/100] | loss train:0.001234, test:0.047183 | lr:0.010000
Epoch[29/100] | loss train:0.001318, test:0.026288 | lr:0.010000
Epoch[30/100] | loss train:0.001263, test:0.027045 | lr:0.010000
Epoch[31/100] | loss train:0.001394, test:0.035512 | lr:0.010000
Epoch[32/100] | loss train:0.001224, test:0.048502 | lr:0.010000
Epoch[33/100] | loss train:0.001181, test:0.052951 | lr:0.010000
Epoch[34/100] | loss train:0.001302, test:0.050365 | lr:0.010000
Epoch[35/100] | loss train:0.001247, test:0.018652 | lr:0.010000
Epoch[36/100] | loss train:0.001327, test:0.064474 | lr:0.010000
Epoch[37/100] | loss train:0.001412, test:0.014595 | lr:0.010000
Epoch[38/100] | loss train:0.001268, test:0.024307 | lr:0.010000
Epoch[39/100] | loss train:0.001174, test:0.066398 | lr:0.010000
Epoch[40/100] | loss train:0.001195, test:0.032992 | lr:0.010000
Epoch[41/100] | loss train:0.000995, test:0.033510 | lr:0.001000
Epoch[42/100] | loss train:0.001006, test:0.034000 | lr:0.001000
Epoch[43/100] | loss train:0.001023, test:0.024088 | lr:0.001000
Epoch[44/100] | loss train:0.000981, test:0.025672 | lr:0.001000
Epoch[45/100] | loss train:0.001098, test:0.027241 | lr:0.001000
Epoch[46/100] | loss train:0.001040, test:0.031539 | lr:0.001000
Epoch[47/100] | loss train:0.000932, test:0.032975 | lr:0.001000
Epoch[48/100] | loss train:0.000938, test:0.032879 | lr:0.001000
Epoch[49/100] | loss train:0.000924, test:0.040343 | lr:0.001000
Epoch[50/100] | loss train:0.000920, test:0.036744 | lr:0.001000
Epoch[51/100] | loss train:0.000922, test:0.033317 | lr:0.001000
Epoch[52/100] | loss train:0.000980, test:0.028740 | lr:0.001000
Epoch[53/100] | loss train:0.000983, test:0.035532 | lr:0.001000
Epoch[54/100] | loss train:0.001090, test:0.031191 | lr:0.001000
Epoch[55/100] | loss train:0.000957, test:0.039110 | lr:0.001000
Epoch[56/100] | loss train:0.000998, test:0.036136 | lr:0.001000
Epoch[57/100] | loss train:0.000977, test:0.043983 | lr:0.001000
Epoch[58/100] | loss train:0.000965, test:0.043374 | lr:0.001000
Epoch[59/100] | loss train:0.000905, test:0.044103 | lr:0.001000
Epoch[60/100] | loss train:0.000949, test:0.038137 | lr:0.001000
Epoch[61/100] | loss train:0.000942, test:0.035683 | lr:0.001000
Epoch[62/100] | loss train:0.001007, test:0.035598 | lr:0.001000
Epoch[63/100] | loss train:0.000977, test:0.031050 | lr:0.001000
Epoch[64/100] | loss train:0.000898, test:0.032657 | lr:0.001000
Epoch[65/100] | loss train:0.000970, test:0.037500 | lr:0.001000
Epoch[66/100] | loss train:0.000966, test:0.031084 | lr:0.001000
Epoch[67/100] | loss train:0.000961, test:0.026193 | lr:0.001000
Epoch[68/100] | loss train:0.001001, test:0.021706 | lr:0.001000
Epoch[69/100] | loss train:0.000927, test:0.030507 | lr:0.001000
Epoch[70/100] | loss train:0.000942, test:0.040249 | lr:0.001000
Epoch[71/100] | loss train:0.000975, test:0.025638 | lr:0.001000
Epoch[72/100] | loss train:0.001001, test:0.030100 | lr:0.001000
Epoch[73/100] | loss train:0.000977, test:0.026130 | lr:0.001000
Epoch[74/100] | loss train:0.000958, test:0.027025 | lr:0.001000
Epoch[75/100] | loss train:0.000943, test:0.030233 | lr:0.001000
Epoch[76/100] | loss train:0.000912, test:0.033821 | lr:0.001000
Epoch[77/100] | loss train:0.000900, test:0.027913 | lr:0.001000
Epoch[78/100] | loss train:0.000955, test:0.026191 | lr:0.001000
Epoch[79/100] | loss train:0.000942, test:0.024198 | lr:0.001000
Epoch[80/100] | loss train:0.000980, test:0.026549 | lr:0.001000
Epoch[81/100] | loss train:0.000926, test:0.027217 | lr:0.000100
Epoch[82/100] | loss train:0.000946, test:0.027862 | lr:0.000100
Epoch[83/100] | loss train:0.000888, test:0.027857 | lr:0.000100
Epoch[84/100] | loss train:0.001031, test:0.027484 | lr:0.000100
Epoch[85/100] | loss train:0.000929, test:0.027524 | lr:0.000100
Epoch[86/100] | loss train:0.000923, test:0.030329 | lr:0.000100
Epoch[87/100] | loss train:0.000973, test:0.028228 | lr:0.000100
Epoch[88/100] | loss train:0.000932, test:0.027897 | lr:0.000100
Epoch[89/100] | loss train:0.000887, test:0.028695 | lr:0.000100
Epoch[90/100] | loss train:0.000923, test:0.028859 | lr:0.000100
Epoch[91/100] | loss train:0.000933, test:0.029310 | lr:0.000100
Epoch[92/100] | loss train:0.000947, test:0.028618 | lr:0.000100
Epoch[93/100] | loss train:0.000901, test:0.030970 | lr:0.000100
Epoch[94/100] | loss train:0.000871, test:0.030632 | lr:0.000100
Epoch[95/100] | loss train:0.000940, test:0.032207 | lr:0.000100
Epoch[96/100] | loss train:0.000946, test:0.030894 | lr:0.000100
Epoch[97/100] | loss train:0.000899, test:0.030930 | lr:0.000100
Epoch[98/100] | loss train:0.000865, test:0.032610 | lr:0.000100
Epoch[99/100] | loss train:0.000951, test:0.032280 | lr:0.000100
Epoch[100/100] | loss train:0.000920, test:0.031189 | lr:0.000100
Predicted close price of the next trading day: 149.31
