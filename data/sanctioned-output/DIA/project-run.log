All libraries loaded
config["alpha_vantage"]["datafile"] is data/alphavantage_TIME_SERIES_DAILY_ADJUSTED__DIA__data.json
*********** TEST MODE ***********
     disabling all randomness
*********** TEST MODE ***********
Number data points 6333 from 1999-11-01 to 2025-01-02
Train data shape (5050, 20, 1) (5050,)
Validation data shape (1263, 20, 1) (1263,)
Epoch[1/100] | loss train:0.024822, test:0.135753 | lr:0.010000
Epoch[2/100] | loss train:0.003931, test:0.096997 | lr:0.010000
Epoch[3/100] | loss train:0.003744, test:0.058919 | lr:0.010000
Epoch[4/100] | loss train:0.003570, test:0.085106 | lr:0.010000
Epoch[5/100] | loss train:0.002998, test:0.078381 | lr:0.010000
Epoch[6/100] | loss train:0.002235, test:0.077824 | lr:0.010000
Epoch[7/100] | loss train:0.003004, test:0.054279 | lr:0.010000
Epoch[8/100] | loss train:0.002662, test:0.068523 | lr:0.010000
Epoch[9/100] | loss train:0.002824, test:0.076709 | lr:0.010000
Epoch[10/100] | loss train:0.002613, test:0.055422 | lr:0.010000
Epoch[11/100] | loss train:0.002794, test:0.084745 | lr:0.010000
Epoch[12/100] | loss train:0.002315, test:0.059274 | lr:0.010000
Epoch[13/100] | loss train:0.002842, test:0.112664 | lr:0.010000
Epoch[14/100] | loss train:0.002585, test:0.075448 | lr:0.010000
Epoch[15/100] | loss train:0.002030, test:0.066245 | lr:0.010000
Epoch[16/100] | loss train:0.002154, test:0.057869 | lr:0.010000
Epoch[17/100] | loss train:0.002519, test:0.046532 | lr:0.010000
Epoch[18/100] | loss train:0.002439, test:0.057427 | lr:0.010000
Epoch[19/100] | loss train:0.002307, test:0.053949 | lr:0.010000
Epoch[20/100] | loss train:0.002660, test:0.056335 | lr:0.010000
Epoch[21/100] | loss train:0.001956, test:0.048744 | lr:0.010000
Epoch[22/100] | loss train:0.002270, test:0.052304 | lr:0.010000
Epoch[23/100] | loss train:0.002249, test:0.048486 | lr:0.010000
Epoch[24/100] | loss train:0.002040, test:0.063528 | lr:0.010000
Epoch[25/100] | loss train:0.002231, test:0.074462 | lr:0.010000
Epoch[26/100] | loss train:0.002188, test:0.038292 | lr:0.010000
Epoch[27/100] | loss train:0.002273, test:0.049865 | lr:0.010000
Epoch[28/100] | loss train:0.002225, test:0.048491 | lr:0.010000
Epoch[29/100] | loss train:0.002081, test:0.066401 | lr:0.010000
Epoch[30/100] | loss train:0.002386, test:0.053242 | lr:0.010000
Epoch[31/100] | loss train:0.002285, test:0.029687 | lr:0.010000
Epoch[32/100] | loss train:0.002200, test:0.054606 | lr:0.010000
Epoch[33/100] | loss train:0.001967, test:0.035404 | lr:0.010000
Epoch[34/100] | loss train:0.002486, test:0.041093 | lr:0.010000
Epoch[35/100] | loss train:0.002136, test:0.039003 | lr:0.010000
Epoch[36/100] | loss train:0.002028, test:0.047898 | lr:0.010000
Epoch[37/100] | loss train:0.002086, test:0.059555 | lr:0.010000
Epoch[38/100] | loss train:0.002090, test:0.050548 | lr:0.010000
Epoch[39/100] | loss train:0.002246, test:0.049722 | lr:0.010000
Epoch[40/100] | loss train:0.002106, test:0.060301 | lr:0.010000
Epoch[41/100] | loss train:0.001822, test:0.066999 | lr:0.001000
Epoch[42/100] | loss train:0.001695, test:0.068339 | lr:0.001000
Epoch[43/100] | loss train:0.001633, test:0.057059 | lr:0.001000
Epoch[44/100] | loss train:0.001771, test:0.047372 | lr:0.001000
Epoch[45/100] | loss train:0.001657, test:0.049767 | lr:0.001000
Epoch[46/100] | loss train:0.001598, test:0.049393 | lr:0.001000
Epoch[47/100] | loss train:0.001682, test:0.055047 | lr:0.001000
Epoch[48/100] | loss train:0.001576, test:0.056181 | lr:0.001000
Epoch[49/100] | loss train:0.001806, test:0.063630 | lr:0.001000
Epoch[50/100] | loss train:0.001606, test:0.061738 | lr:0.001000
Epoch[51/100] | loss train:0.001692, test:0.053807 | lr:0.001000
Epoch[52/100] | loss train:0.001617, test:0.062098 | lr:0.001000
Epoch[53/100] | loss train:0.001727, test:0.058730 | lr:0.001000
Epoch[54/100] | loss train:0.001604, test:0.055000 | lr:0.001000
Epoch[55/100] | loss train:0.001659, test:0.052172 | lr:0.001000
Epoch[56/100] | loss train:0.001720, test:0.054667 | lr:0.001000
Epoch[57/100] | loss train:0.001634, test:0.057714 | lr:0.001000
Epoch[58/100] | loss train:0.001622, test:0.062131 | lr:0.001000
Epoch[59/100] | loss train:0.001681, test:0.058772 | lr:0.001000
Epoch[60/100] | loss train:0.001656, test:0.059699 | lr:0.001000
Epoch[61/100] | loss train:0.001714, test:0.052537 | lr:0.001000
Epoch[62/100] | loss train:0.001592, test:0.052230 | lr:0.001000
Epoch[63/100] | loss train:0.001545, test:0.051075 | lr:0.001000
Epoch[64/100] | loss train:0.001597, test:0.047634 | lr:0.001000
Epoch[65/100] | loss train:0.001526, test:0.058712 | lr:0.001000
Epoch[66/100] | loss train:0.001588, test:0.046841 | lr:0.001000
Epoch[67/100] | loss train:0.001508, test:0.053306 | lr:0.001000
Epoch[68/100] | loss train:0.001634, test:0.053298 | lr:0.001000
Epoch[69/100] | loss train:0.001596, test:0.047380 | lr:0.001000
Epoch[70/100] | loss train:0.001595, test:0.052197 | lr:0.001000
Epoch[71/100] | loss train:0.001683, test:0.058436 | lr:0.001000
Epoch[72/100] | loss train:0.001662, test:0.056696 | lr:0.001000
Epoch[73/100] | loss train:0.001649, test:0.072529 | lr:0.001000
Epoch[74/100] | loss train:0.001656, test:0.057284 | lr:0.001000
Epoch[75/100] | loss train:0.001570, test:0.057043 | lr:0.001000
Epoch[76/100] | loss train:0.001569, test:0.048594 | lr:0.001000
Epoch[77/100] | loss train:0.001636, test:0.062986 | lr:0.001000
Epoch[78/100] | loss train:0.001600, test:0.066526 | lr:0.001000
Epoch[79/100] | loss train:0.001641, test:0.048003 | lr:0.001000
Epoch[80/100] | loss train:0.001658, test:0.048954 | lr:0.001000
Epoch[81/100] | loss train:0.001512, test:0.053203 | lr:0.000100
Epoch[82/100] | loss train:0.001541, test:0.053620 | lr:0.000100
Epoch[83/100] | loss train:0.001614, test:0.052948 | lr:0.000100
Epoch[84/100] | loss train:0.001468, test:0.051107 | lr:0.000100
Epoch[85/100] | loss train:0.001593, test:0.051881 | lr:0.000100
Epoch[86/100] | loss train:0.001528, test:0.051079 | lr:0.000100
Epoch[87/100] | loss train:0.001549, test:0.052638 | lr:0.000100
Epoch[88/100] | loss train:0.001466, test:0.055258 | lr:0.000100
Epoch[89/100] | loss train:0.001621, test:0.055208 | lr:0.000100
Epoch[90/100] | loss train:0.001532, test:0.052302 | lr:0.000100
Epoch[91/100] | loss train:0.001508, test:0.053141 | lr:0.000100
Epoch[92/100] | loss train:0.001541, test:0.054300 | lr:0.000100
Epoch[93/100] | loss train:0.001557, test:0.053724 | lr:0.000100
Epoch[94/100] | loss train:0.001513, test:0.052445 | lr:0.000100
Epoch[95/100] | loss train:0.001471, test:0.053906 | lr:0.000100
Epoch[96/100] | loss train:0.001624, test:0.055996 | lr:0.000100
Epoch[97/100] | loss train:0.001529, test:0.052703 | lr:0.000100
Epoch[98/100] | loss train:0.001580, test:0.055879 | lr:0.000100
Epoch[99/100] | loss train:0.001458, test:0.052289 | lr:0.000100
Epoch[100/100] | loss train:0.001566, test:0.055244 | lr:0.000100
Predicted close price of the next trading day: 328.16
